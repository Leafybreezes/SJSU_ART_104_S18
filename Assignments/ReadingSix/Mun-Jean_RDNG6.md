Jean Mun
Professor A. Blanton- ART 104
March 7, 2018

               Reading 6: “Invisible Images (Your Pictures Are Looking at You)” by Trevor Paglen
               
	This reading highlights a significant expansion in conventional visual cultures as it became intertwined 
  with machine vision. The classical approach to our visual culture has shifted with images beginning to
  “intervene in everyday life, their functions changing from representation and mediation, to activations,
  operations, and enforcement”. It is no longer strictly a human-human visual culture. Human-human visual 
  culture can be free to communicate “counter-hegemonic visual strategies and tactics” with the ambiguity 
  that is a trait of human interpretation; it can challenge inequality and injustices. For instance, Martha
  Rosler’s “Semiotics of the Kitchen” is a work that she created with an intention to transform the 
  “patriarchal image of the kitchen as a representation of masculinist order into a kind of prison”. 
  Human-human visual strategies have a reliance on our understanding of the flexibility that ambiguity
  allows human interpretations. We have approached the influences of our visual culture in these ways for
  centuries, yet we have not entirely grasped that this semiotic approach is non-existent in a machine-machine
  system. The fact that a digital image and its form is non-materialistic, means that it is not something you 
  can have with you if your phone is off, “reverting back to its immaterial machine form when the phone is put
  away or the display is turned off”. A photograph is a machine-readable file that does not communicate the 
  same visual explanation for the human interpretation. We as humans do not accept the type of file the machine
  takes unless it converts it for us to be something we can view and perceive as a physical possession. To 
  elaborate, a roll of underdeveloped film must go through a chemical process to become visible by the human eye.
  Until this process is under went, the film is not readable by a machine or a human. A picture on one’s phone may
  not be within one’s access but a machine does not require it to be in “human readable form” in order for 
  something to be done with it. Paglen is addressing the fact that digital images are machine-readable whether or
  not we are involved. He mentions how is allows an “automation of vision” on a massive scale that we should be 
  conscious of (despite its non-physical nature). Once images are introduced digitally, we are acknowledging that
  it can be taken without the owner’s consent. The greatest issue of privacy comes from knowing one is introducing 
  information about oneself to extremely powerful artificial intelligence that is taking and growing from every 
  contributor assisting the objective it was created for; strengthening neural networks. Despite these facts
  influencing a analogous characteristic, Paglen mentions how “neural networks cannot invent their own classes; 
  they’re only able to relate images they ingest to images that they’ve been trained on”. Paglen stresses that
  “their training sets reveal the historical, geographical, racial, and socio-economic positions of their 
  trainers”. He references an example of how an image of Manet’s painting “Olympia” was interpreted as a burrito 
  once it run through a CNN trained on Imagenet. Or when engineers chose to deactivate the “gorilla class” upon 
  realizing that the algorithm was identifying African Americans as apes due to being predominantly trained to
  interpret Caucasian faces. The rebuttal to this is to simply eliminate the bias by focusing on accuracy for 
  training programs. Paglen’s argument is alerting the fact that algorithmic operations cannot achieve the 
  human-human level of moral standards. Paglen is addressing the fact that we cannot rely on a machine to work on
  being less biased to be fair and equal. Even as it approaches greater accuracy, it will be objective in completing
  what it was programmed to do. We are feeding these platforms with information that is organizing its interpretations
  of you. Not with an intention to harm, but with an objective to take the information and to further develop its
  neural networks. Regardless of the fact that the machine is not coming from a place of intentional harm, it is still 
  gathering all this personal data about us that can be taken to manipulate us. “The invisible world of images isn’t
  simply an alternative taxonomy of visuality. It is an active, cunning, exercise of power, one ideally suited to
  molecular police and market operations–one designed to insert its tendrils into ever-smaller slices of everyday 
  life”. For example, Vigilant Solutions have an arrangement with the police by providing them with their database 
  in exchange for the police providing records of outstanding arrest warrants and overdue court fees. “Flagged” license
  plates are then fed into ALPR systems that allow police to stop these people to pay immediately or to go to jail. It 
  is not perceived as preying on the weak; as being counterproductive to socio-economic level struggles. Paglen clarifies:
The political operations here are clear. Municipalities are incentivized to balance their budgets on the backs of their
most vulnerable populations, to transform their police into tax-collectors, and to effectively sell police surveillance
data to private companies. Despite the “objectivity” of the overall system, it unambiguously serves powerful government
and corporate interests at the expense of the most vulnerable.

